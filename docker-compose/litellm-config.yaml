# litellm config yml

model_list:
  - model_name: "*" # all requests where model not in your config go to this deployment
    litellm_params:
      model: openai/* # set `openai/` to use the openai route
      api_key: os.environ/OPENAI_API_KEY

litellm_settings: # module level litellm settings - https://github.com/BerriAI/litellm/blob/main/litellm/__init__.py
  drop_params: True
  success_callback: ["langfuse"] # OPTIONAL - if you want to start sending LLM Logs to Langfuse. Make sure to set `LANGFUSE_PUBLIC_KEY` and `LANGFUSE_SECRET_KEY` in your env

general_settings:
  master_key: sk-1234 # [OPTIONAL] Only use this if you to require all calls to contain this key (Authorization: Bearer sk-1234)
